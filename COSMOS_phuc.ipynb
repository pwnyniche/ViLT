{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9963f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143d9c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class SentenceNegator:\n",
    "  IRREGULAR_ES_VERB_ENDINGS = [\"ss\", \"x\", \"ch\", \"sh\", \"o\"]\n",
    "\n",
    "  def negate(self, sentence):\n",
    "    # is\n",
    "    if sentence.find(\"isn't\") > -1:\n",
    "      return sentence.replace(\"isn't\", \"is\")\n",
    "\n",
    "    if sentence.find(\"isn\\\\'t\") > -1:\n",
    "      return sentence.replace(\"isn\\\\'t\", \"is\")\n",
    "\n",
    "    if sentence.find(\"is not \") > -1:\n",
    "      return sentence.replace(\"is not \", \"is \")\n",
    "\n",
    "    if sentence.find(\"is \") > -1:\n",
    "      return sentence.replace(\"is \", \"is not \")\n",
    "\n",
    "    # has\n",
    "    if sentence.find(\"does not have\") > -1:\n",
    "      return sentence.replace(\"does not have\", \"has\")\n",
    "\n",
    "    if sentence.find(\"doesn't have\") > -1:\n",
    "      return sentence.replace(\"doesn't have\", \"has\")\n",
    "\n",
    "    if sentence.find(\"doesn\\\\'t have\") > -1:\n",
    "      return sentence.replace(\"doesn\\\\'t have\", \"has\")\n",
    "\n",
    "    if sentence.find(\"has \") > -1:\n",
    "      return sentence.replace(\"has \", \"does not have \")\n",
    "\n",
    "    # should\n",
    "    if sentence.find(\"shouldn't\") > -1:\n",
    "      return sentence.replace(\"shouldn't\", \"should\")\n",
    "\n",
    "    if sentence.find(\"shouldn\\\\'t\") > -1:\n",
    "      return sentence.replace(\"shouldn\\\\'t\", \"should\")\n",
    "\n",
    "    if sentence.find(\"should not\") > -1:\n",
    "      return sentence.replace(\"should not\", \"should\")\n",
    "\n",
    "    if sentence.find(\"should\") > -1:\n",
    "      return sentence.replace(\"should\", \"should not\")\n",
    "\n",
    "    # must\n",
    "    if sentence.find(\"mustn't\") > -1:\n",
    "      return sentence.replace(\"mustn't\", \"must\")\n",
    "\n",
    "    if sentence.find(\"mustn\\\\'t\") > -1:\n",
    "      return sentence.replace(\"mustn\\\\'t\", \"must\")\n",
    "\n",
    "    if sentence.find(\"must not\") > -1:\n",
    "      return sentence.replace(\"must not\", \"must\")\n",
    "\n",
    "    if sentence.find(\"must \") > -1:\n",
    "      return sentence.replace(\"must \", \"must not \")\n",
    "\n",
    "    # can\n",
    "    if sentence.find(\"can't\") > -1:\n",
    "      return sentence.replace(\"can't\", \"can\")\n",
    "\n",
    "    if sentence.find(\"can\\\\'t\") > -1:\n",
    "      return sentence.replace(\"can\\\\'t\", \"can\")\n",
    "\n",
    "    if sentence.find(\"cannot\") > -1:\n",
    "      return sentence.replace(\"cannot\", \"can\")\n",
    "\n",
    "    if sentence.find(\"can \") > -1:\n",
    "      return sentence.replace(\"can \", \"cannot \")\n",
    "    # was\n",
    "    if sentence.find(\" was \") > -1:\n",
    "      return sentence.replace(\" was \", \" was not \")\n",
    "    if sentence.find(\"was not \") > -1:\n",
    "      return sentence.replace(\"was not \", \"was \")\n",
    "    if sentence.find(\"wasn't\") > -1:\n",
    "      return sentence.replace(\"wasn't\", \"was\")\n",
    "    # doesn't work -> works\n",
    "    doesnt_regex = r'(doesn\\'t|doesn\\\\\\'t|does not) (?P<verb>\\w+)'\n",
    "\n",
    "    if re.search(doesnt_regex, sentence):\n",
    "      def replace_doesnt(matchobj):\n",
    "        verb = matchobj.group(2)\n",
    "\n",
    "        if verb.endswith(\"y\") and self.__is_consonant(verb[-2]):\n",
    "          return \"{0}ies\".format(verb[0:-1])\n",
    "\n",
    "        for ending in self.IRREGULAR_ES_VERB_ENDINGS:\n",
    "          if verb.endswith(ending):\n",
    "            return \"{0}es\".format(verb)\n",
    "\n",
    "        return \"{0}s\".format(verb)\n",
    "\n",
    "      return re.sub(doesnt_regex, replace_doesnt, sentence, 1)\n",
    "\n",
    "    verb_regex = r'(It |it |)(?P<verb>\\w+)s( |$)'\n",
    "\n",
    "    # works -> does not work\n",
    "    def replace_verb(matchobj):\n",
    "      subject = matchobj.group(1)\n",
    "      verb = matchobj.group(2)\n",
    "      whitespace = matchobj.group(3)\n",
    "\n",
    "      # flies -> fly, but not die -> dy\n",
    "      if verb.endswith(\"ie\") and len(verb) > 3:\n",
    "        verb = \"{0}y\".format(verb[0:-2])\n",
    "\n",
    "      # stresses -> stress\n",
    "      for ending in self.IRREGULAR_ES_VERB_ENDINGS:\n",
    "        if verb.endswith(\"{0}e\".format(ending)):\n",
    "          verb = verb[0:-1]\n",
    "\n",
    "      return \"{0}does not {1}{2}\".format(subject, verb, whitespace)\n",
    "\n",
    "    if re.search(verb_regex, sentence):\n",
    "      return re.sub(verb_regex, replace_verb, sentence, 1)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "  def __is_consonant(self, letter):\n",
    "    return letter not in ['a', 'e', 'i', 'o', 'u', 'y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dd889f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 s, sys: 858 ms, total: 17.5 s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "root = '.'\n",
    "\n",
    "train_data = list(\n",
    "    map(json.loads, open(f\"{root}/cosmos/train_data.json\").readlines())\n",
    ")\n",
    "test_data = list(\n",
    "    map(json.loads, open(f\"{root}/cosmos/test_data.json\").readlines())\n",
    ")\n",
    "val_data = list(map(json.loads, open(f\"{root}/cosmos/val_data.json\").readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5f25b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161754/161754 [00:05<00:00, 31161.49it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# train_data_sample = np.random.choice(train_data, size=int(len(train_data)*50/100))\n",
    "# Not OOC cases\n",
    "sn = SentenceNegator()\n",
    "l = []\n",
    "\n",
    "nice=0\n",
    "ne=0\n",
    "bad=0\n",
    "\n",
    "for data in tqdm(train_data):\n",
    "    if len(data['articles']) > 1:\n",
    "        # Pick 2 random caption\n",
    "        caption_1_idx, caption_2_idx = random.sample(range(0,len(data['articles'])),2)\n",
    "        cap1 = data['articles'][caption_1_idx]['caption_modified']\n",
    "        cap2 = data['articles'][caption_2_idx]['caption_modified']\n",
    "        if cap1 != cap2:\n",
    "            l.append([data['img_local_path'],[cap1],[cap2],[False]])\n",
    "            nice += 1\n",
    "        else:\n",
    "            cap2 = sn.negate(cap2)\n",
    "            if cap1 != cap2:\n",
    "                l.append([data['img_local_path'],[cap1],[cap2],[True]])\n",
    "                ne+=1\n",
    "\n",
    "    if len(data['articles']) == 1 or np.random.rand() > 0:\n",
    "        if np.random.rand()>0.5:\n",
    "            data_random1 = train_data[np.random.randint(len(train_data))]\n",
    "            while data['img_local_path'] == data_random1['img_local_path']:\n",
    "                data_random1 = train_data[np.random.randint(len(train_data))]\n",
    "            cap1 = data_random1['articles'][np.random.randint(len(data_random1['articles']))]['caption_modified']\n",
    "            data_random2 = train_data[np.random.randint(len(train_data))]\n",
    "            while data['img_local_path'] == data_random2['img_local_path']:\n",
    "                data_random2 = train_data[np.random.randint(len(train_data))]\n",
    "            cap2 = data_random2['articles'][np.random.randint(len(data_random2['articles']))]['caption_modified']\n",
    "        else:\n",
    "             # Pick 1st correct caption\n",
    "            if np.random.rand()>0.5:\n",
    "                cap1 = data['articles'][np.random.randint(len(data['articles']))]['caption_modified']\n",
    "                # Pick a random article then pick its first caption\n",
    "                data_random = train_data[np.random.randint(len(train_data))]\n",
    "                while data['img_local_path'] == data_random['img_local_path']:\n",
    "                    data_random = train_data[np.random.randint(len(train_data))]\n",
    "                cap2 = data_random['articles'][np.random.randint(len(data_random['articles']))]['caption_modified']\n",
    "            else:\n",
    "                cap2 = data['articles'][np.random.randint(len(data['articles']))]['caption_modified']\n",
    "                # Pick a random article then pick its first caption\n",
    "                data_random = train_data[np.random.randint(len(train_data))]\n",
    "                if data['img_local_path'] == data_random['img_local_path']:\n",
    "                    continue\n",
    "                cap1 = data_random['articles'][np.random.randint(len(data_random['articles']))]['caption_modified']\n",
    "        l.append([data['img_local_path'],[cap1],[cap2],[True]])\n",
    "        bad+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb49b924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62018, 33678, 161754)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nice,ne,bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b491360",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_train = pd.DataFrame(\n",
    "    l, columns=[\"image\", \"caption_1\", \"caption_2\", \"label\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e25773d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     195432\n",
       "False     62018\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_train['label'].apply(lambda x:x[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1ccdd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "   try:\n",
    "      with open(path, \"rb\") as fp:\n",
    "        return fp.read()\n",
    "   except:\n",
    "      return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20de3ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/vilt/lib/python3.8/site-packages/tqdm/std.py:702: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 61735/61735 [00:10<00:00, 5748.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# tqdm.pandas()\n",
    "\n",
    "# dataframe['image'] = dataframe['image'].progress_apply(lambda x: load_image(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71b1b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe = dataframe[dataframe.image.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab4ed3e",
   "metadata": {},
   "source": [
    "# Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc8ac523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41006/41006 [00:00<00:00, 44500.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(42)\n",
    "# val_data_sample = np.random.choice(val_data, size=int(len(val_data)*50/100))\n",
    "# train_data_sample=train_data[:1]\n",
    "# Not OOC cases\n",
    "nice = 0\n",
    "ne = 0\n",
    "bad = 0\n",
    "l = []\n",
    "for data in tqdm(val_data):\n",
    "    if len(data['articles']) > 1:\n",
    "        # Pick 2 random caption\n",
    "        caption_1_idx, caption_2_idx = random.sample(range(0,len(data['articles'])),2)\n",
    "        cap1 = data['articles'][caption_1_idx]['caption_modified']\n",
    "        cap2 = data['articles'][caption_2_idx]['caption_modified']\n",
    "        if cap1 != cap2:\n",
    "            l.append([data['img_local_path'],[cap1],[cap2],[False]])\n",
    "            nice += 1\n",
    "        else:\n",
    "            cap2 = sn.negate(cap2)\n",
    "            if cap1 != cap2:\n",
    "                l.append([data['img_local_path'],[cap1],[cap2],[True]])\n",
    "                ne += 1\n",
    "\n",
    "    if len(data['articles']) == 1 or np.random.rand() > 0:\n",
    "        if np.random.rand()>0.5:\n",
    "            data_random1 = train_data[np.random.randint(len(train_data))]\n",
    "            while data['img_local_path'] == data_random1['img_local_path']:\n",
    "                data_random1 = train_data[np.random.randint(len(train_data))]\n",
    "            cap1 = data_random1['articles'][np.random.randint(len(data_random1['articles']))]['caption_modified']\n",
    "            data_random2 = train_data[np.random.randint(len(train_data))]\n",
    "            while data['img_local_path'] == data_random2['img_local_path']:\n",
    "                data_random2 = train_data[np.random.randint(len(train_data))]\n",
    "            cap2 = data_random2['articles'][np.random.randint(len(data_random2['articles']))]['caption_modified']\n",
    "        else:\n",
    "             # Pick 1st correct caption\n",
    "            if np.random.rand()>0.5:\n",
    "                cap1 = data['articles'][np.random.randint(len(data['articles']))]['caption_modified']\n",
    "                # Pick a random article then pick its first caption\n",
    "                data_random = train_data[np.random.randint(len(train_data))]\n",
    "                while data['img_local_path'] == data_random['img_local_path']:\n",
    "                    data_random = train_data[np.random.randint(len(train_data))]\n",
    "                cap2 = data_random['articles'][np.random.randint(len(data_random['articles']))]['caption_modified']\n",
    "            else:\n",
    "                cap2 = data['articles'][np.random.randint(len(data['articles']))]['caption_modified']\n",
    "                # Pick a random article then pick its first caption\n",
    "                data_random = train_data[np.random.randint(len(train_data))]\n",
    "                if data['img_local_path'] == data_random['img_local_path']:\n",
    "                    continue\n",
    "                cap1 = data_random['articles'][np.random.randint(len(data_random['articles']))]['caption_modified']\n",
    "        l.append([data['img_local_path'],[cap1],[cap2],[True]])\n",
    "        bad+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "532bee12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15616, 8043, 41006)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nice,ne,bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fd1aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_val = pd.DataFrame(\n",
    "    l, columns=[\"image\", \"caption_1\", \"caption_2\", \"label\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "276d797a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/vilt/lib/python3.8/site-packages/tqdm/std.py:702: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 15588/15588 [00:01<00:00, 14784.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# tqdm.pandas()\n",
    "\n",
    "# dataframe_val['image'] = dataframe_val['image'].progress_apply(lambda x: load_image(x))\n",
    "# dataframe = dataframe[dataframe.image.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e629f",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "614eb437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2c7e7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_caption_replace_entities(caption_text):\n",
    "    \"\"\"\n",
    "        Utility function to replace named entities in the caption with their corresponding hypernyms\n",
    "        Args:\n",
    "            caption_text (str): Original caption with named entities\n",
    "        Returns:\n",
    "            caption_modified (str): Modified caption after replacing named entities\n",
    "    \"\"\"\n",
    "    doc = nlp(caption_text)\n",
    "    caption_modified = caption_text\n",
    "    caption_entity_list = []\n",
    "    for ent in doc.ents:\n",
    "        caption_entity_list.append((ent.text, ent.label_))\n",
    "        caption_modified = caption_modified.replace(ent.text, ent.label_, 1)\n",
    "    return caption_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb1ead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1700/1700 [00:00<00:00, 475512.96it/s]\n"
     ]
    }
   ],
   "source": [
    "l_test  = []\n",
    "for data in tqdm(test_data):\n",
    "    cap1 = (data['caption1'])\n",
    "    cap2 = (data['caption2'])\n",
    "    l_test.append([data['img_local_path'],[cap1],[cap2],[data['context_label']==True]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b8b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_test = pd.DataFrame(\n",
    "    l_test, columns=[\"image\", \"caption_1\", \"caption_2\", \"label\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bb02050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257450, 64665, 1700)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframe_train),len(dataframe_val),len(dataframe_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa7b38b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c352ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_score(cap1,cap2):\n",
    "    cap1 = model.encode(cap1,convert_to_tensor=False)\n",
    "    cap2 = model.encode(cap2,convert_to_tensor=False)\n",
    "    return util.cos_sim(cap1,cap2)[0][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf5d483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm.pandas()\n",
    "\n",
    "# dataframe_test['bert'] = dataframe_test.progress_apply(lambda x: \n",
    "#     bert_score(x.caption_1[0], x.caption_2[0]), axis=1)\n",
    "# dataframe_train['bert'] = dataframe_train.progress_apply(lambda x: \n",
    "#     bert_score(x.caption_1[0], x.caption_2[0]), axis=1)\n",
    "# dataframe_val['bert'] = dataframe_val.progress_apply(lambda x: \n",
    "#     bert_score(x.caption_1[0], x.caption_2[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ba1d0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257450/257450 [00:49<00:00, 5164.01it/s] \n",
      "100%|██████████| 64665/64665 [00:21<00:00, 3012.17it/s]\n",
      "100%|██████████| 1700/1700 [00:01<00:00, 1573.26it/s]\n"
     ]
    }
   ],
   "source": [
    "dataframe_train['image'] = dataframe_train['image'].progress_apply(lambda x: load_image(x))\n",
    "dataframe_val['image'] = dataframe_val['image'].progress_apply(lambda x: load_image(x))\n",
    "dataframe_test['image'] = dataframe_test['image'].progress_apply(lambda x: load_image(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cc20ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "shuffled = dataframe_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da3641cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = shuffled[:1000]\n",
    "val = shuffled[1000:1350]\n",
    "test = shuffled[1350:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219bfdb9",
   "metadata": {},
   "source": [
    "# PyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "250c78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataframe,split in zip([train,val,test],['train','val','test']):\n",
    "#     table = pa.Table.from_pandas(dataframe)\n",
    "#     # split = 'test'\n",
    "#     with pa.OSFile(f\"dataset_test_only/cosmos_{split}.arrow\", \"wb\") as sink:\n",
    "#         with pa.RecordBatchFileWriter(sink, table.schema) as writer:\n",
    "#             writer.write_table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35530120",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(dataframe_test)\n",
    "split = 'test'\n",
    "with pa.OSFile(f\"dataset/normal_cap/cosmos_{split}.arrow\", \"wb\") as sink:\n",
    "    with pa.RecordBatchFileWriter(sink, table.schema) as writer:\n",
    "        writer.write_table(table)\n",
    "\n",
    "# table = pa.Table.from_pandas(dataframe_val)\n",
    "# split = 'val'\n",
    "# with pa.OSFile(f\"dataset/data_neg/cosmos_{split}.arrow\", \"wb\") as sink:\n",
    "#     with pa.RecordBatchFileWriter(sink, table.schema) as writer:\n",
    "#         writer.write_table(table)\n",
    "\n",
    "# table = pa.Table.from_pandas(dataframe_train)\n",
    "# split = 'train'\n",
    "# with pa.OSFile(f\"dataset/data_neg/cosmos_{split}.arrow\", \"wb\") as sink:\n",
    "#     with pa.RecordBatchFileWriter(sink, table.schema) as writer:\n",
    "#         writer.write_table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_tokenizer(from_pretrained):\n",
    "    if torch.distributed.is_initialized():\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            BertTokenizer.from_pretrained(\n",
    "                from_pretrained, do_lower_case=\"uncased\" in from_pretrained\n",
    "            )\n",
    "        torch.distributed.barrier()\n",
    "    return BertTokenizer.from_pretrained(\n",
    "        from_pretrained, do_lower_case=\"uncased\" in from_pretrained\n",
    "    )\n",
    "tokenizer = get_pretrained_tokenizer(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d7af4",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed24acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vilt.datasets.base_dataset import BaseDataset\n",
    "class COSMOSDataset(BaseDataset):\n",
    "    def __init__(self, *args, split=\"\", **kwargs):\n",
    "        assert split in [\"train\", \"val\", \"test\"]\n",
    "        self.split = split\n",
    "\n",
    "        if split == \"train\":\n",
    "            names = [\"cosmos_train\"]\n",
    "        elif split == \"val\":\n",
    "            names = [\"cosmos_val\", \"cosmos_test\"]\n",
    "        elif split == \"test\":\n",
    "            names = [\"cosmos_val\", \"cosmos_test\"]\n",
    "\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "            names=names,\n",
    "            text_column_name=\"caption_1\",\n",
    "            remove_duplicate=False,\n",
    "        )\n",
    "    \n",
    "    def get_text_2(self, raw_index):\n",
    "        index, caption_index = self.index_mapper[raw_index]\n",
    "        text = self.table['caption_2'][index][0].as_py()\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_len,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "        return {\n",
    "            \"text\": (text, encoding),\n",
    "            \"img_index\": index,\n",
    "            \"cap_index\": caption_index,\n",
    "            \"raw_index\": raw_index,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        result = None\n",
    "        while result is None:\n",
    "            try:\n",
    "                image_tensor = self.get_image(index, image_key=\"image\")[\"image\"]\n",
    "                text = self.get_text(index)[\"text\"]\n",
    "                text2 = self.get_text_2(index)[\"text\"]\n",
    "                result = True\n",
    "            except:\n",
    "                print(\n",
    "                    f\"error while read file idx {index} in {self.names[0]}\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "                z\n",
    "                index = random.randint(0, len(self.index_mapper) - 1)\n",
    "\n",
    "        return {\n",
    "            \"image\": image_tensor,\n",
    "            \"text\": text,\n",
    "            \"text2\": text2,\n",
    "            \"answers\": self.table[\"label\"][index] == True,\n",
    "            \"table_name\": self.table_names[index],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a431314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer from\n",
    "import torch\n",
    "from transformers import (\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForWholeWordMask,\n",
    "    BertTokenizer,\n",
    ")\n",
    "\n",
    "def get_pretrained_tokenizer(from_pretrained):\n",
    "    if torch.distributed.is_initialized():\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            BertTokenizer.from_pretrained(\n",
    "                from_pretrained, do_lower_case=\"uncased\" in from_pretrained\n",
    "            )\n",
    "        torch.distributed.barrier()\n",
    "    return BertTokenizer.from_pretrained(\n",
    "        from_pretrained, do_lower_case=\"uncased\" in from_pretrained\n",
    "    )\n",
    "tokenizer = get_pretrained_tokenizer(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "187162f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = COSMOSDataset(data_dir='dataset', split='train', transform_keys=[\"pixelbert_randaug\"],image_size=384)\n",
    "n.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fa2abee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Mr. Zuckerberg wants to increase the utility of the social network to keep Facebook's billions of users highly engaged, people involved in the effort said.\",\n",
       " {'input_ids': [101, 2720, 1012, 16950, 9102, 4059, 4122, 2000, 3623, 1996, 9710, 1997, 1996, 2591, 2897, 2000, 2562, 9130, 1005, 1055, 25501, 1997, 5198, 3811, 5117, 1010, 2111, 2920, 1999, 1996, 3947, 2056, 1012, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n[1]['text2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af3a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd6ac6d03954cf65a3aa8ce4cedda9aac0145a018ad67eca07aa9c761ef41ce4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('vilt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
