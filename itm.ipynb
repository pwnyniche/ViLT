{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "from PIL import Image\n",
    "import pyarrow as pa\n",
    "from vilt.config import ex\n",
    "from vilt.modules import ViLTransformerSS\n",
    "\n",
    "from vilt.modules.objectives import cost_matrix_cosine, ipot\n",
    "from vilt.transforms import pixelbert_transform\n",
    "from vilt.datamodules.datamodule_base import get_pretrained_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pa.ipc.RecordBatchFileReader(\n",
    "    pa.memory_map(f\"dataset_50/cosmos_test.arrow\", \"r\")\n",
    ").read_all().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_config = {'exp_name': 'vilt', 'seed': 0, 'datasets': ['coco'], 'loss_names': {'itm': 1, 'mlm': 0, 'mpp': 0, 'vqa': 0, 'nlvr2': 0, 'irtr': 0, 'cosmos': 0}, 'batch_size': 4096, 'train_transform_keys': ['pixelbert'], 'val_transform_keys': ['pixelbert'], 'image_size': 384, 'max_image_len': -1, 'patch_size': 32, 'draw_false_image': 1, 'image_only': False, 'vqav2_label_size': 3129, 'max_text_len': 40, 'tokenizer': 'bert-base-uncased', 'vocab_size': 30522, 'whole_word_masking': False, 'mlm_prob': 0.15, 'draw_false_text': 0, 'vit': 'vit_base_patch32_384', 'hidden_size': 768, 'num_heads': 12, 'num_layers': 12, 'mlp_ratio': 4, 'drop_rate': 0.1, 'optim_type': 'adamw', 'learning_rate': 0.0001, 'weight_decay': 0.01, 'decay_power': 1, 'max_epoch': 100, 'max_steps': 25000, 'warmup_steps': 2500, 'end_lr': 0, 'lr_mult': 1, 'get_recall_metric': False, 'resume_from': None, 'fast_dev_run': False, 'val_check_interval': 1.0, 'test_only': False, 'data_root': '', 'log_dir': 'result', 'per_gpu_batchsize': 0, 'num_gpus': 1, 'num_nodes': 1, \n",
    "    'load_path': 'weights/vilt_200k_mlm_itm.ckpt', 'num_workers': 8, 'precision': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_pretrained_tokenizer(_config[\"tokenizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViLTransformerSS(_config)\n",
    "model.setup(\"test\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['mlm_score.bias', 'mlm_score.transform.dense.weight', 'mlm_score.transform.dense.bias', 'mlm_score.transform.LayerNorm.weight', 'mlm_score.transform.LayerNorm.bias', 'mlm_score.decoder.weight'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(_config[\"load_path\"], map_location=\"cpu\")\n",
    "state_dict = ckpt[\"state_dict\"]\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda:0\" if _config[\"num_gpus\"] > 0 else \"cpu\"\n",
    "device=\"cuda:0\"\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_image(img_byte):\n",
    "    image_bytes = io.BytesIO(img_byte)\n",
    "    image_bytes.seek(0)\n",
    "    return Image.open(image_bytes).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "def infer(image,text,text2):\n",
    "    img = pixelbert_transform(size=384)(image)\n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    batch = {\"text\": [text], \"image\": [img]}\n",
    "    batch2 = {\"text\": [text2], \"image\": [img]}\n",
    "    with torch.no_grad():\n",
    "        encoded = tokenizer(batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=40,\n",
    "        return_special_tokens_mask=True)\n",
    "        batch[\"text_ids\"] = torch.tensor(encoded[\"input_ids\"]).to(device)\n",
    "        batch[\"text_labels\"] = torch.tensor(encoded[\"input_ids\"]).to(device)\n",
    "        batch[\"text_masks\"] = torch.tensor(encoded[\"attention_mask\"]).to(device)\n",
    "\n",
    "        encoded2 = tokenizer(batch2[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=40,\n",
    "        return_special_tokens_mask=True)\n",
    "        batch2[\"text_ids\"] = torch.tensor(encoded2[\"input_ids\"]).to(device)\n",
    "        batch2[\"text_labels\"] = torch.tensor(encoded2[\"input_ids\"]).to(device)\n",
    "        batch2[\"text_masks\"] = torch.tensor(encoded2[\"attention_mask\"]).to(device)\n",
    "\n",
    "        infer1 = model.infer(batch)\n",
    "        infer2 = model.infer(batch2)\n",
    "\n",
    "        # cls_feats = torch.cat([infer1[\"cls_feats\"], infer2[\"cls_feats\"]], dim=-1)\n",
    "        itm_logits1 = model.itm_score(infer1[\"cls_feats\"])\n",
    "        itm_logits2 = model.itm_score(infer2[\"cls_feats\"])\n",
    "        softmax = nn.Softmax()\n",
    "        itm_logits1 = softmax(itm_logits1)\n",
    "        itm_logits2 = softmax(itm_logits2)\n",
    "        # cosmos_logits = model.nlvr2_classifier(cls_feats)\n",
    "    # encoded = encoded[\"input_ids\"][0][1:-1]\n",
    "    # inferred_token = [tokenizer.decode(encoded)]\n",
    "    # return infer1,inferred_token\n",
    "    return itm_logits1, itm_logits2\n",
    "    # return not(itm_logits1.argmax().item() and itm_logits2.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n",
      "Men from the LOC tribe perform a traditional jumping ritual as they observe a rite of passage to mark the transition to cultural junior elder within the Masai-Mara national reserve.\n",
      "And on DATE in GPE's Narok county, young PERSON men take part in initiation rites to become moran - the men who are traditionally the warrior class.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/vilt2/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/root/anaconda3/envs/vilt2/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0022, 0.9978]], device='cuda:0'),\n",
       " tensor([[0.0210, 0.9790]], device='cuda:0'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = df.iloc[4]\n",
    "print(r['label'])\n",
    "print( r['caption_1'][0])\n",
    "print( r['caption_2'][0])\n",
    "result = infer(get_raw_image(r['image']), r['caption_1'][0], r['caption_2'][0])\n",
    "# result = infer(get_raw_image(r['image']), 'Person playing', 'People with dogs playing with snow')\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1700 [00:00<?, ?it/s]/root/anaconda3/envs/vilt2/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/root/anaconda3/envs/vilt2/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 1700/1700 [09:20<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame({},columns=['predict','label','cap1','cap2'])\n",
    "for i in tqdm(np.arange(len(df))):\n",
    "    r = df.iloc[i]\n",
    "    pred = infer(get_raw_image(r['image']), r['caption_1'][0], r['caption_2'][0])\n",
    "    # s = util.cos_sim(model2.encode(r['caption_1'][0]),(model2.encode(r['caption_2'][0])))\n",
    "    pred = pred == True\n",
    "    ans = r['label'][0]\n",
    "    result.loc[len(result)] = [pred, ans,r['caption_1'][0],r['caption_2'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual     False  True \n",
      "Predicted              \n",
      "False        651    568\n",
      "True         199    282\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(result['predict'], result['label'], rownames=['Predicted'], colnames=['Actual'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6064705882352941"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(680+351)/1700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual     False  True \n",
      "Predicted              \n",
      "False        677    493\n",
      "True         173    357\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(result['predict'], result['label'], rownames=['Predicted'], colnames=['Actual'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6082352941176471"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(677 + 357) / 1700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9a86a7ed262735ebe154e2af3d12815b689f8138287f40a62edb0c9d1ca95e9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('vilt2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
