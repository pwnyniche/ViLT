{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "from PIL import Image\n",
    "import pyarrow as pa\n",
    "from vilt.config import ex\n",
    "from vilt.modules import ViLTransformerSS\n",
    "\n",
    "from vilt.modules.objectives import cost_matrix_cosine, ipot\n",
    "from vilt.transforms import pixelbert_transform\n",
    "from vilt.datamodules.datamodule_base import get_pretrained_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pa.ipc.RecordBatchFileReader(\n",
    "    pa.memory_map(f\"dataset_50/cosmos_test.arrow\", \"r\")\n",
    ").read_all().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_config = {'exp_name': 'vilt', 'seed': 0, 'datasets': ['coco'], 'loss_names': {'itm': 1, 'mlm': 0, 'mpp': 0, 'vqa': 0, 'nlvr2': 0, 'irtr': 0, 'cosmos': 0}, 'batch_size': 4096, 'train_transform_keys': ['pixelbert'], 'val_transform_keys': ['pixelbert'], 'image_size': 384, 'max_image_len': -1, 'patch_size': 32, 'draw_false_image': 1, 'image_only': False, 'vqav2_label_size': 3129, 'max_text_len': 40, 'tokenizer': 'bert-base-uncased', 'vocab_size': 30522, 'whole_word_masking': False, 'mlm_prob': 0.15, 'draw_false_text': 0, 'vit': 'vit_base_patch32_384', 'hidden_size': 768, 'num_heads': 12, 'num_layers': 12, 'mlp_ratio': 4, 'drop_rate': 0.1, 'optim_type': 'adamw', 'learning_rate': 0.0001, 'weight_decay': 0.01, 'decay_power': 1, 'max_epoch': 100, 'max_steps': 25000, 'warmup_steps': 2500, 'end_lr': 0, 'lr_mult': 1, 'get_recall_metric': False, 'resume_from': None, 'fast_dev_run': False, 'val_check_interval': 1.0, 'test_only': False, 'data_root': '', 'log_dir': 'result', 'per_gpu_batchsize': 0, 'num_gpus': 1, 'num_nodes': 1, \n",
    "    'load_path': 'weights/vilt_200k_mlm_itm.ckpt', 'num_workers': 8, 'precision': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_pretrained_tokenizer(_config[\"tokenizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViLTransformerSS(_config)\n",
    "model.setup(\"test\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['mlm_score.bias', 'mlm_score.transform.dense.weight', 'mlm_score.transform.dense.bias', 'mlm_score.transform.LayerNorm.weight', 'mlm_score.transform.LayerNorm.bias', 'mlm_score.decoder.weight'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(_config[\"load_path\"], map_location=\"cpu\")\n",
    "state_dict = ckpt[\"state_dict\"]\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda:0\" if _config[\"num_gpus\"] > 0 else \"cpu\"\n",
    "device=\"cuda:0\"\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_image(img_byte):\n",
    "    image_bytes = io.BytesIO(img_byte)\n",
    "    image_bytes.seek(0)\n",
    "    return Image.open(image_bytes).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "def infer(image,text,text2):\n",
    "    img = pixelbert_transform(size=384)(image)\n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    batch = {\"text\": [text], \"image\": [img]}\n",
    "    batch2 = {\"text\": [text2], \"image\": [img]}\n",
    "    with torch.no_grad():\n",
    "        encoded = tokenizer(batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=40,\n",
    "        return_special_tokens_mask=True)\n",
    "        batch[\"text_ids\"] = torch.tensor(encoded[\"input_ids\"]).to(device)\n",
    "        batch[\"text_labels\"] = torch.tensor(encoded[\"input_ids\"]).to(device)\n",
    "        batch[\"text_masks\"] = torch.tensor(encoded[\"attention_mask\"]).to(device)\n",
    "\n",
    "        encoded2 = tokenizer(batch2[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=40,\n",
    "        return_special_tokens_mask=True)\n",
    "        batch2[\"text_ids\"] = torch.tensor(encoded2[\"input_ids\"]).to(device)\n",
    "        batch2[\"text_labels\"] = torch.tensor(encoded2[\"input_ids\"]).to(device)\n",
    "        batch2[\"text_masks\"] = torch.tensor(encoded2[\"attention_mask\"]).to(device)\n",
    "\n",
    "        infer1 = model.infer(batch)\n",
    "        infer2 = model.infer(batch2)\n",
    "\n",
    "        # cls_feats = torch.cat([infer1[\"cls_feats\"], infer2[\"cls_feats\"]], dim=-1)\n",
    "        itm_logits1 = model.itm_score(infer1[\"cls_feats\"])\n",
    "        itm_logits2 = model.itm_score(infer2[\"cls_feats\"])\n",
    "        softmax = nn.Softmax()\n",
    "        itm_logits1 = softmax(itm_logits1)\n",
    "        itm_logits2 = softmax(itm_logits2)\n",
    "        # cosmos_logits = model.nlvr2_classifier(cls_feats)\n",
    "    # encoded = encoded[\"input_ids\"][0][1:-1]\n",
    "    # inferred_token = [tokenizer.decode(encoded)]\n",
    "    # return infer1,inferred_token\n",
    "    return itm_logits1[0][1].item(), itm_logits2[0][1].item()\n",
    "    # return not(itm_logits1.argmax().item() and itm_logits2.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/root/thesis/dataset/public_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "   with open(path, \"rb\") as fp:\n",
    "      return fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/vilt2/lib/python3.7/site-packages/tqdm/std.py:702: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1645.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "tqdm.pandas()\n",
    "data['image_byte'] = data['img_local_path'].progress_apply(lambda x: load_image(\n",
    "    os.path.join('/root/thesis/dataset',x)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itm(row):\n",
    "    c1 = row['caption1_modified'][0]\n",
    "    c2 = row['caption2_modified'][0]\n",
    "    c1_itm, c2_itm = infer(get_raw_image(row['image_byte']), c1, c2)\n",
    "    row['c1_itm'] = c1_itm\n",
    "    row['c2_itm'] = c2_itm\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/root/anaconda3/envs/vilt2/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/root/anaconda3/envs/vilt2/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 1000/1000 [02:18<00:00,  7.23it/s]\n"
     ]
    }
   ],
   "source": [
    "data = data.progress_apply(itm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['img_local_path','c1_itm','c2_itm']].to_csv('itm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c1_itm    0.345793\n",
       "c2_itm    0.170505\n",
       "Name: 837, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['c1_itm','c2_itm']].loc[837]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n",
      "On DATE zebras cross the road as a motorcyclist waits in ORG in GPE.\n",
      "Zebras cross the road which has QUANTITY-long free electric-fence area for all animals as their corridor in the NORP-Tsavo ecosystem at TIME as a motorbike waits next to ORG in GPE, GPE, on DATE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/vilt2/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/root/anaconda3/envs/vilt2/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.41710996627807617, 1.2162016901129391e-05)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = df.iloc[159]\n",
    "print(r['label'])\n",
    "print( r['caption_1'][0])\n",
    "print( r['caption_2'][0])\n",
    "# result = infer(get_raw_image(r['image']), r['caption_1'][0], r['caption_2'][0])\n",
    "result = infer(get_raw_image(r['image']), 'zerba flying', 'People with dogs playing with snow')\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1700 [00:00<?, ?it/s]/root/anaconda3/envs/vilt2/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/root/anaconda3/envs/vilt2/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 1700/1700 [09:20<00:00,  3.03it/s]\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame({},columns=['predict','label','cap1','cap2'])\n",
    "for i in tqdm(np.arange(len(df))):\n",
    "    r = df.iloc[i]\n",
    "    pred = infer(get_raw_image(r['image']), r['caption_1'][0], r['caption_2'][0])\n",
    "    # s = util.cos_sim(model2.encode(r['caption_1'][0]),(model2.encode(r['caption_2'][0])))\n",
    "    pred = pred == True\n",
    "    ans = r['label'][0]\n",
    "    result.loc[len(result)] = [pred, ans,r['caption_1'][0],r['caption_2'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual     False  True \n",
      "Predicted              \n",
      "False        651    568\n",
      "True         199    282\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(result['predict'], result['label'], rownames=['Predicted'], colnames=['Actual'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6064705882352941"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(680+351)/1700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual     False  True \n",
      "Predicted              \n",
      "False        677    493\n",
      "True         173    357\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(result['predict'], result['label'], rownames=['Predicted'], colnames=['Actual'])\n",
    "print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6082352941176471"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(677 + 357) / 1700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9a86a7ed262735ebe154e2af3d12815b689f8138287f40a62edb0c9d1ca95e9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('vilt2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
